{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d09a6893",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import os\n",
    "import sys\n",
    "from tqdm.auto import tqdm\n",
    "from matplotlib.colors import LogNorm\n",
    "import pickle\n",
    "\n",
    "\n",
    "cwd = os.getcwd()\n",
    "parent_dir = os.path.dirname(cwd)\n",
    "base_dir = os.path.dirname(os.path.dirname(cwd))\n",
    "src_dir = base_dir + \"/src\"\n",
    "\n",
    "sys.path.insert(0, src_dir)\n",
    "\n",
    "from calibration import MPEM, MPEM_AVAILABLE, LinearNet, ActuationNet, DirectNet, PotentialNet, DirectGBT\n",
    "from evaluate import ModelPhantom, EvaluationPackage, get_affine_fits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e38d4c6",
   "metadata": {},
   "source": [
    "## Define paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eeac56b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#############################\n",
    "######## Which eMNS? ########\n",
    "#############################\n",
    "\n",
    "emns = \"octomag\" # \"octomag\" or \"navion\"\n",
    "\n",
    "if emns == \"octomag\":\n",
    "    poly_fits_dir = parent_dir + \"/gradients/data/\"\n",
    "    data_dir = base_dir + \"/data/octomag_data/split_dataset/\"\n",
    "    nn_params_dir = parent_dir + \"/training/params/\"\n",
    "    gbt_params_dir = parent_dir + \"/training/trees/\"\n",
    "    assembled_data_dir = cwd + \"/evaluation_packages/\"\n",
    "    mpem_params_dir = base_dir + \"/mpem/optimized_parameters/\"\n",
    "elif emns == \"navion\":\n",
    "    poly_fits_dir = parent_dir + \"/gradients/data_navion/\"\n",
    "    data_dir = base_dir + \"/data/navion_data/split_dataset/\"\n",
    "    nn_params_dir = parent_dir + \"/training/params_navion/\"\n",
    "    gbt_params_dir = parent_dir + \"/training/trees/navion_trees/\"\n",
    "    assembled_data_dir = cwd + \"/evaluation_packages/navion/\"\n",
    "    mpem_params_dir = base_dir + \"/mpem/mpem_navion/optimized_parameters/\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25dc35a3",
   "metadata": {},
   "source": [
    "## Load evaluation data package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8eb7859a",
   "metadata": {},
   "outputs": [],
   "source": [
    "##############################\n",
    "######## Package name ########\n",
    "##############################\n",
    "test_package_name = \"test_eval_pack.pkl\"\n",
    "training_package_name = \"train_eval_pack.pkl\"\n",
    "\n",
    "# Check if test set package exists\n",
    "test_package_path = assembled_data_dir + test_package_name\n",
    "if os.path.exists(test_package_path):\n",
    "    print(f\"Loading existing test set evaluation package from {test_package_path}...\") \n",
    "    test_package = EvaluationPackage.load_from(test_package_path)\n",
    "else:\n",
    "    # Create new\n",
    "    print(f\"Creating new test set evaluation package...\")\n",
    "    test_package = EvaluationPackage()\n",
    "\n",
    "# Check if training set package exists\n",
    "training_package_path = assembled_data_dir + training_package_name\n",
    "if os.path.exists(training_package_path):\n",
    "    print(f\"Loading existing training set evaluation package from {training_package_path}...\") \n",
    "    training_package = EvaluationPackage.load_from(training_package_path)\n",
    "else:\n",
    "    # Create new\n",
    "    print(f\"Creating new training set evaluation package...\")\n",
    "    training_package = EvaluationPackage()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4accafee",
   "metadata": {},
   "source": [
    "## Load new models to gather evals from"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2787172d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load models to session\n",
    "nn_models = [\n",
    "    ActuationNet.load_from(nn_params_dir + \"ActuationNet_100_256x256.pt\"),\n",
    "    ActuationNet.load_from(nn_params_dir + \"ActuationNet_100_256x256x256.pt\"),\n",
    "    ActuationNet.load_from(nn_params_dir + \"ActuationNet_100_512x512x512.pt\"),\n",
    "    ActuationNet.load_from(nn_params_dir + \"ActuationNet_50_512x512x512.pt\"),\n",
    "    ActuationNet.load_from(nn_params_dir + \"ActuationNet_20_512x512x512.pt\"),\n",
    "    ActuationNet.load_from(nn_params_dir + \"ActuationNet_5_512x512x512.pt\"),\n",
    "    ActuationNet.load_from(nn_params_dir + \"ActuationNet_1_512x512x512.pt\"),\n",
    "    DirectNet.load_from(nn_params_dir + \"/DirectNet_100_256x256.pt\"),\n",
    "    DirectNet.load_from(nn_params_dir + \"/DirectNet_100_256x256x256.pt\"),\n",
    "    DirectNet.load_from(nn_params_dir + \"/DirectNet_100_512x512x512.pt\"),\n",
    "    DirectNet.load_from(nn_params_dir + \"/DirectNet_50_512x512x512.pt\"),\n",
    "    DirectNet.load_from(nn_params_dir + \"/DirectNet_20_512x512x512.pt\"),\n",
    "    DirectNet.load_from(nn_params_dir + \"/DirectNet_5_512x512x512.pt\"),\n",
    "    DirectNet.load_from(nn_params_dir + \"/DirectNet_1_512x512x512.pt\"),\n",
    "    PotentialNet.load_from(nn_params_dir + \"/PotentialNet_100_256x256.pt\"),\n",
    "    PotentialNet.load_from(nn_params_dir + \"/PotentialNet_100_256x256x256.pt\"),\n",
    "    PotentialNet.load_from(nn_params_dir + \"/PotentialNet_100_512x512x512.pt\"),\n",
    "    PotentialNet.load_from(nn_params_dir + \"/PotentialNet_50_512x512x512.pt\"),\n",
    "    PotentialNet.load_from(nn_params_dir + \"/PotentialNet_20_512x512x512.pt\"),\n",
    "    PotentialNet.load_from(nn_params_dir + \"/PotentialNet_5_512x512x512.pt\"),\n",
    "    PotentialNet.load_from(nn_params_dir + \"/PotentialNet_1_512x512x512.pt\"),\n",
    "]\n",
    "\n",
    "tree_models = [\n",
    "    DirectGBT.load_from(gbt_params_dir + \"/DirectGBT_100_32.gbt.zip\"),\n",
    "    DirectGBT.load_from(gbt_params_dir + \"/DirectGBT_100_64.gbt.zip\"),\n",
    "    DirectGBT.load_from(gbt_params_dir + \"/DirectGBT_100_128.gbt.zip\"),\n",
    "    DirectGBT.load_from(gbt_params_dir + \"/DirectGBT_50_128.gbt.zip\"),\n",
    "    DirectGBT.load_from(gbt_params_dir + \"/DirectGBT_20_128.gbt.zip\"),\n",
    "    DirectGBT.load_from(gbt_params_dir + \"/DirectGBT_5_128.gbt.zip\"),\n",
    "    DirectGBT.load_from(gbt_params_dir + \"/DirectGBT_1_128.gbt.zip\"),\n",
    "]\n",
    "\n",
    "\n",
    "mpem_models = []\n",
    "if MPEM_AVAILABLE :\n",
    "    \n",
    "    mpem_param_files = {\n",
    "        \"MPEM_5_3\": \"optimized_octopole_5.yaml\",\n",
    "        \"MPEM_1_3\": \"optimized_octopole_1.yaml\",\n",
    "        \"MPEM_5_2\": \"optimized_quadrupole_5.yaml\",\n",
    "        \"MPEM_5_1\": \"optimized_dipole_5.yaml\",\n",
    "    }\n",
    "\n",
    "    for model_name, param_file in mpem_param_files.items(): \n",
    "        mpem_models.append(MPEM(mpem_params_dir + param_file, model_name))\n",
    "\n",
    "\n",
    "models = nn_models + tree_models + mpem_models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "881b3d26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load models into package\n",
    "for model in models:\n",
    "    test_package.load_model(model, force=True)\n",
    "    training_package.load_model(model, force=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09ecd674",
   "metadata": {},
   "source": [
    "## Load data into package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a9556a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data_base = \"training_data\"\n",
    "test_data_base = \"test_data\"\n",
    "\n",
    "data_percentages = [100, 50, 20, 5, 1]\n",
    "\n",
    "overwrite = True\n",
    "\n",
    "\n",
    "# Load test data. No reduction here.\n",
    "test_data = pd.read_pickle(data_dir + test_data_base + \".pkl\"   )\n",
    "pos_cols = ['x', 'y', 'z']\n",
    "current_cols = [col for col in test_data.columns if col.startswith('em_')]\n",
    "field_cols = ['Bx', 'By', 'Bz']\n",
    "\n",
    "# Extract positions, currents, and fields cols from dataframes\n",
    "test_positions = test_data[pos_cols].to_numpy()\n",
    "test_currents = test_data[current_cols].to_numpy()\n",
    "test_fields = test_data[field_cols].to_numpy()\n",
    "\n",
    "# Load data into package\n",
    "print(f\"Loading full test data into evaluation package...\")\n",
    "test_package.load_features_and_labels(test_positions, test_currents, test_fields, dataset_percentage=100, overwrite=overwrite)\n",
    "\n",
    "\n",
    "for percentage in data_percentages:\n",
    "    training_data_file = f\"{training_data_base}_{percentage}.pkl\" if percentage != 100 else f\"{training_data_base}.pkl\"\n",
    "\n",
    "    training_data = pd.read_pickle(data_dir + training_data_file)\n",
    "\n",
    "    # Extract positions, currents, and fields cols from dataframes  \n",
    "    training_positions = training_data[pos_cols].to_numpy()\n",
    "    training_currents = training_data[current_cols].to_numpy()\n",
    "    training_fields = training_data[field_cols].to_numpy()\n",
    "\n",
    "    # Load data into package\n",
    "    print(f\"Loading training data with percentage {percentage} into training package...\")\n",
    "    training_package.load_features_and_labels(training_positions, training_currents, training_fields, dataset_percentage=percentage, overwrite=overwrite)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff2741d9",
   "metadata": {},
   "source": [
    "## Compute field predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4aee1e46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For test dataset\n",
    "test_package.compute_field_predictions(keep_existing=True)\n",
    "\n",
    "# Store package\n",
    "print(\"Storing test evaluation package...\\n\")\n",
    "test_package.store(assembled_data_dir + test_package_name)\n",
    "print(\"Successfully stored test evaluation package.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d20153da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For training dataset\n",
    "training_package.compute_field_predictions(keep_existing=True)\n",
    "\n",
    "# Store package\n",
    "print(\"Storing training evaluation package...\\n\")   \n",
    "training_package.store(assembled_data_dir + training_package_name)\n",
    "print(\"Successfully stored training evaluation package.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b243f298",
   "metadata": {},
   "source": [
    "## Compute gradient predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71fc247b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For test dataset\n",
    "test_package.compute_gradient_predictions(keep_existing=False)\n",
    "\n",
    "# Store package\n",
    "print(\"Storing test evaluation package with gradient predictions...\\n\")\n",
    "test_package.store(assembled_data_dir + test_package_name)\n",
    "print(\"Successfully stored test evaluation package with gradient predictions.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b486978",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For training dataset\n",
    "training_package.compute_gradient_predictions(keep_existing=False)\n",
    "\n",
    "# Store package\n",
    "print(\"Storing training evaluation package...\\n\")\n",
    "training_package.store(assembled_data_dir + training_package_name)\n",
    "print(\"Successfully stored training evaluation package.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed7e93de",
   "metadata": {},
   "source": [
    "## Load gradient references from polynomial fits\n",
    "\n",
    "Only required if more complex gradient evaluation metrics than curl magnitude and divergence are used.\n",
    "\n",
    "In the paper, and in this project's evaluation notebooks, we use only these metrics, and therefore do not require gradients references from polynomial fits to be computed. In any case, these can be obtained using the notebook in `notebooks/gradients/poly_fit.ipynb`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cccba73d",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set_poly_fits_file = \"poly_grads_test.pkl\"\n",
    "training_set_poly_fits_file = \"poly_grads_training.pkl\"\n",
    "\n",
    "# Load polynomial fits\n",
    "try:\n",
    "    with open(poly_fits_dir + test_set_poly_fits_file, \"rb\") as f:\n",
    "        test_set_poly_fits = pickle.load(f)\n",
    "    with open(poly_fits_dir + training_set_poly_fits_file, \"rb\") as f:  \n",
    "        training_set_poly_fits = pickle.load(f)\n",
    "\n",
    "    # Choose which order polynomial fit, and nearest neighbor support count to use\n",
    "    order = 1\n",
    "    nn_count = 125\n",
    "\n",
    "    # Add polynomial fits to packages\n",
    "    test_package.load_gradient_references(test_set_poly_fits)\n",
    "    training_package.load_gradient_references(training_set_poly_fits)\n",
    "\n",
    "    # Store packages with polynomial fit references\n",
    "    print(\"Storing test evaluation package with polynomial fit references...\\n\")\n",
    "    test_package.store(assembled_data_dir + test_package_name)\n",
    "    print(\"Successfully stored test evaluation package with polynomial fit references.\")\n",
    "    print(\"Storing training evaluation package with polynomial fit references...\\n\")\n",
    "    training_package.store(assembled_data_dir + training_package_name)\n",
    "    print(\"Successfully stored training evaluation package with polynomial fit references.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error loading polynomial fits: {e}. Proceding without adding polynomial fit references.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c2f40e6",
   "metadata": {},
   "source": [
    "## Get and load affine fits per position for reference\n",
    "\n",
    "Useful for computing floor metrics for a \"perfect\" affine map estimator. Was not used in the paper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8aa0d79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute affine fit for each position in test set?\n",
    "recompute_test_affine_fits = False\n",
    "\n",
    "if recompute_test_affine_fits:\n",
    "    test_preds, test_fits = get_affine_fits(test_data, resolution=float(0.005/3.0))\n",
    "\n",
    "    test_affine_phantom = ModelPhantom(name=\"AffinePerPos\")\n",
    "\n",
    "    test_package.manual_load_field_predictions(test_affine_phantom, test_preds)\n",
    "\n",
    "    # Store package\n",
    "    print(\"Storing test evaluation package with affine per position fits...\\n\")\n",
    "    test_package.store(assembled_data_dir + test_package_name)\n",
    "    print(\"Successfully stored test evaluation package with affine per position fits.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5aed9ab5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute affine fit for each position in training set?\n",
    "recompute_training_affine_fits = False\n",
    "\n",
    "if recompute_training_affine_fits:\n",
    "    training_preds, training_fits = get_affine_fits(training_data, resolution=float(0.005/3.0))\n",
    "\n",
    "    training_affine_phantom = ModelPhantom(name=\"AffinePerPos\")\n",
    "\n",
    "    training_package.manual_load_field_predictions(training_affine_phantom, training_preds)\n",
    "    # Store package\n",
    "    print(\"Storing training evaluation package with affine per position fits...\\n\")\n",
    "    training_package.store(assembled_data_dir + training_package_name)\n",
    "    print(\"Successfully stored training evaluation package with affine per position fits.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "calibration",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
