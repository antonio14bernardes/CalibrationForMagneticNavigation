{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7efec6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "cwd = os.getcwd()\n",
    "parent_dir = os.path.dirname(cwd)\n",
    "base_dir = os.path.dirname(parent_dir)\n",
    "src_dir = os.path.join(base_dir, \"src\")\n",
    "package_dir = os.path.join(parent_dir, \"testing\", \"evaluation_packages\")\n",
    "plot_dir = cwd + \"/plots/\"\n",
    "\n",
    "sys.path.insert(0, src_dir)\n",
    "\n",
    "from evaluate import ModelPhantom, EvaluationPackage, metrics # From main src dir \n",
    "from paper import latex_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceaaa450",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_package_name = \"test_eval_pack.pkl\"\n",
    "training_package_name = \"train_eval_pack.pkl\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c41015f",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_package, training_package = None, None\n",
    "\n",
    "try:\n",
    "    print(\"Loading test set evaluation packages...\")\n",
    "    test_package = EvaluationPackage.load_from(package_dir + \"/\" +test_package_name)\n",
    "    print(\"Successfully loaded test set evaluation packages.\\n\")\n",
    "except Exception as e:\n",
    "    print(f\"Error loading test set evaluation package: {e}\\n\")\n",
    "\n",
    "try:\n",
    "    print(\"Loading training set evaluation packages...\")\n",
    "    training_package = EvaluationPackage.load_from(package_dir + \"/\" + training_package_name)\n",
    "    print(\"Successfully loaded training set evaluation packages.\\n\")\n",
    "except Exception as e:\n",
    "    print(f\"Error loading training set evaluation package: {e}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecbb4453",
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################\n",
    "####### Which metrics? ########\n",
    "###############################\n",
    "\n",
    "metrics_list = [\n",
    "    metrics.rse,\n",
    "    metrics.mag_and_angle, # Not actually used in the paper\n",
    "]\n",
    "\n",
    "##############################\n",
    "####### Which models? ########\n",
    "##############################\n",
    "\n",
    "models = [\n",
    "    ModelPhantom(name=\"ActuationNet\", dataset_percentage=100, structure=(512, 512, 512)),\n",
    "    ModelPhantom(name=\"ActuationNet\", dataset_percentage=100, structure=(256, 256, 256)),\n",
    "    ModelPhantom(name=\"ActuationNet\", dataset_percentage=100, structure=(256, 256)),\n",
    "    ModelPhantom(name=\"ActuationNet\", dataset_percentage=50, structure=(512, 512, 512)),\n",
    "    ModelPhantom(name=\"ActuationNet\", dataset_percentage=20, structure=(512, 512, 512)),\n",
    "    ModelPhantom(name=\"ActuationNet\", dataset_percentage=5, structure=(512, 512, 512)),\n",
    "    ModelPhantom(name=\"ActuationNet\", dataset_percentage=1, structure=(512, 512, 512)),\n",
    "    ModelPhantom(name=\"PotentialNet\", dataset_percentage=100, structure=(512, 512, 512)),\n",
    "    ModelPhantom(name=\"PotentialNet\", dataset_percentage=100, structure=(256, 256, 256)),\n",
    "    ModelPhantom(name=\"PotentialNet\", dataset_percentage=100, structure=(256, 256)),\n",
    "    ModelPhantom(name=\"PotentialNet\", dataset_percentage=50, structure=(512, 512, 512)),\n",
    "    ModelPhantom(name=\"PotentialNet\", dataset_percentage=20, structure=(512, 512, 512)),\n",
    "    ModelPhantom(name=\"PotentialNet\", dataset_percentage=5, structure=(512, 512, 512)),\n",
    "    ModelPhantom(name=\"PotentialNet\", dataset_percentage=1, structure=(512, 512, 512)),\n",
    "    ModelPhantom(name=\"DirectNet\", dataset_percentage=100, structure=(512, 512, 512)),\n",
    "    ModelPhantom(name=\"DirectNet\", dataset_percentage=100, structure=(256, 256, 256)),\n",
    "    ModelPhantom(name=\"DirectNet\", dataset_percentage=100, structure=(256, 256)),\n",
    "    ModelPhantom(name=\"DirectNet\", dataset_percentage=50, structure=(512, 512, 512)),\n",
    "    ModelPhantom(name=\"DirectNet\", dataset_percentage=20, structure=(512, 512, 512)),\n",
    "    ModelPhantom(name=\"DirectNet\", dataset_percentage=5, structure=(512, 512, 512)),\n",
    "    ModelPhantom(name=\"DirectNet\", dataset_percentage=1, structure=(512, 512, 512)),\n",
    "    ModelPhantom(name=\"DirectGBT\", dataset_percentage=100, structure=(128,)),\n",
    "    ModelPhantom(name=\"DirectGBT\", dataset_percentage=100, structure=(64,)),\n",
    "    ModelPhantom(name=\"DirectGBT\", dataset_percentage=100, structure=(32,)),\n",
    "    ModelPhantom(name=\"DirectGBT\", dataset_percentage=50, structure=(128,)),\n",
    "    ModelPhantom(name=\"DirectGBT\", dataset_percentage=20, structure=(128,)),\n",
    "    ModelPhantom(name=\"DirectGBT\", dataset_percentage=5, structure=(128,)),\n",
    "    ModelPhantom(name=\"DirectGBT\", dataset_percentage=1, structure=(128,)),\n",
    "    ModelPhantom(name=\"MPEM\", dataset_percentage=5, structure=(1,)),\n",
    "    ModelPhantom(name=\"MPEM\", dataset_percentage=5, structure=(2,)),\n",
    "    ModelPhantom(name=\"MPEM\", dataset_percentage=5, structure=(3,)),\n",
    "    ModelPhantom(name=\"MPEM\", dataset_percentage=1, structure=(3,)),\n",
    "\n",
    "]\n",
    "\n",
    "full_models = [model for model in models if (model.dataset_percentage == 100 or model.name == \"MPEM\" and model.dataset_percentage == 5)]\n",
    "large_models = [model for model in models if model.structure == (512, 512, 512) or model.structure == (128,) or model.structure == (3,)]\n",
    "large_full_models = [model for model in large_models if model.dataset_percentage == 100]\n",
    "\n",
    "# Compute metrics\n",
    "for metric in metrics_list:\n",
    "    print(f\"Computing {metric.__name__} for test set...\")\n",
    "    test_package.apply_field_metric(metric)\n",
    "    print(f\"Computing {metric.__name__} for training set...\")\n",
    "    training_package.apply_field_metric(metric)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4e5d39e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_train_test_rmse_rank_and_pct_suite_latex(\n",
    "    train_pkg,\n",
    "    test_pkg,\n",
    "    models_rank,\n",
    "    models_pct,\n",
    "    *,\n",
    "    title=None,\n",
    "    figsize=(10, 4.8),\n",
    "    ylabel_test=r\"Test RMSE (mT)\",\n",
    "    ylabel_gap=r\"Test $-$ Train RMSE (mT)\",\n",
    "    xlabel_rank=r\"Model complexity rank\",\n",
    "    xlabel_pct=r\"Dataset percentage (\\%)\",\n",
    "    marker=\"o\",\n",
    "    ymin_test=None,\n",
    "    ymax_test=None,\n",
    "    ymin_gap=None,\n",
    "    ymax_gap=None,\n",
    "    legend_out=True,\n",
    "    legend_ncol=1,\n",
    "    latex_params=None,      # e.g. latex_prms_2img or latex_prms_3img\n",
    "    usetex=True,\n",
    "    panel_labels=(\"a\", \"b\", \"c\", \"d\"),\n",
    "    panel_label_y_top=-0.15,\n",
    "    panel_label_y_bot=-0.38,\n",
    "    # visual grouping controls\n",
    "    column_titles=(r\"vs model complexity\", r\"vs dataset percentage\"),\n",
    "    show_column_titles=True,\n",
    "\n",
    "    # --- legend sizing control ---\n",
    "    legend_match_axes_labelsize=True, \n",
    "    legend_fontsize=None,               # override (takes precedence)\n",
    "\n",
    "    # --- pass a fixed name->color mapping (kept as-is for overlapping models) ---\n",
    "    name_to_color=None,                 # dict: { \"ActuationNet\": (r,g,b), ... } or {name: \"C0\", ...}\n",
    "\n",
    "    store_to=\"rmse_suite.pdf\"\n",
    "):\n",
    "    \n",
    "    def _metric_leaf(pkg, phantom):\n",
    "        name, dataset_percentage, structure = phantom.keys()\n",
    "        try:\n",
    "            return pkg.field_metrics[name][int(dataset_percentage)][structure]\n",
    "        except Exception as e:\n",
    "            raise KeyError(\n",
    "                f\"Missing field metrics for model '{phantom.string(verbose=True)}'. \"\n",
    "                f\"Tried dataset_percentage keys: {dataset_percentage!r}, {int(dataset_percentage)!r}, {str(dataset_percentage)!r}\"\n",
    "            ) from e\n",
    "\n",
    "    # ----------------------------\n",
    "    # Build DF for rank panel\n",
    "    # ----------------------------\n",
    "    rows_rank = []\n",
    "    for ph in models_rank:\n",
    "        name, dataset_percentage, structure = ph.keys()\n",
    "        leaf_tr = _metric_leaf(train_pkg, ph)\n",
    "        leaf_te = _metric_leaf(test_pkg, ph)\n",
    "\n",
    "        rmse_train = float(leaf_tr[\"rmse\"])\n",
    "        rmse_test  = float(leaf_te[\"rmse\"])\n",
    "\n",
    "        structure_tuple = None if structure is None else tuple(structure)\n",
    "        size_score = 1.0 if structure_tuple is None else float(np.prod(np.asarray(structure_tuple, dtype=np.float64)))\n",
    "\n",
    "        rows_rank.append({\n",
    "            \"name\": name,\n",
    "            \"dataset_percentage\": int(dataset_percentage),\n",
    "            \"structure\": structure_tuple,\n",
    "            \"size_score\": size_score,\n",
    "            \"rmse_train\": rmse_train,\n",
    "            \"rmse_test\": rmse_test,\n",
    "            \"rmse_gap\": rmse_test - rmse_train,\n",
    "        })\n",
    "\n",
    "    df_rank = pd.DataFrame(rows_rank)\n",
    "    if df_rank.empty:\n",
    "        raise ValueError(\"models_rank produced no rows (empty list or missing rmse).\")\n",
    "\n",
    "    df_rank = df_rank.sort_values([\"name\", \"size_score\", \"structure\"], na_position=\"last\").reset_index(drop=True)\n",
    "    df_rank[\"size_rank\"] = df_rank.groupby(\"name\").cumcount() + 1\n",
    "\n",
    "    # ----------------------------\n",
    "    # Build DF for pct panel\n",
    "    # ----------------------------\n",
    "    rows_pct = []\n",
    "    for ph in models_pct:\n",
    "        name, dataset_percentage, structure = ph.keys()\n",
    "        leaf_tr = _metric_leaf(train_pkg, ph)\n",
    "        leaf_te = _metric_leaf(test_pkg, ph)\n",
    "\n",
    "        rmse_train = float(leaf_tr[\"rmse\"])\n",
    "        rmse_test  = float(leaf_te[\"rmse\"])\n",
    "\n",
    "        structure_tuple = None if structure is None else tuple(structure)\n",
    "        line_id = name if structure_tuple is None else f\"{name}_{'x'.join(map(str, structure_tuple))}\"\n",
    "\n",
    "        rows_pct.append({\n",
    "            \"name\": name,\n",
    "            \"line_id\": line_id,\n",
    "            \"dataset_percentage\": int(dataset_percentage),\n",
    "            \"structure\": structure_tuple,\n",
    "            \"rmse_train\": rmse_train,\n",
    "            \"rmse_test\": rmse_test,\n",
    "            \"rmse_gap\": rmse_test - rmse_train,\n",
    "        })\n",
    "\n",
    "    df_pct = pd.DataFrame(rows_pct)\n",
    "    if df_pct.empty:\n",
    "        raise ValueError(\"models_pct produced no rows (empty list or missing rmse).\")\n",
    "\n",
    "    df_pct = df_pct.sort_values([\"line_id\", \"dataset_percentage\"]).reset_index(drop=True)\n",
    "\n",
    "    # ----------------------------\n",
    "    # Palette keyed by model name\n",
    "    # ----------------------------\n",
    "    model_names = sorted(set(df_rank[\"name\"].unique()).union(set(df_pct[\"name\"].unique())))\n",
    "    sns_palette = sns.color_palette(\"deep\", n_colors=len(model_names))\n",
    "\n",
    "    if name_to_color is None:\n",
    "        palette = {nm: sns_palette[i] for i, nm in enumerate(model_names)}\n",
    "    else:\n",
    "        palette = dict(name_to_color)  # don't mutate caller\n",
    "        for i, nm in enumerate(model_names):\n",
    "            if nm not in palette:\n",
    "                palette[nm] = sns_palette[i]\n",
    "\n",
    "    # ----------------------------\n",
    "    # Legend order = order models were passed in\n",
    "    # ----------------------------\n",
    "    legend_order = []\n",
    "    for ph in list(models_rank) + list(models_pct):\n",
    "        nm = ph.keys()[0]\n",
    "        if nm not in legend_order:\n",
    "            legend_order.append(nm)\n",
    "\n",
    "    # ----------------------------\n",
    "    # Plotting (inside LaTeX rc_context)\n",
    "    # ----------------------------\n",
    "    latex_params = {} if latex_params is None else dict(latex_params)\n",
    "    latex_params.setdefault(\"figsize\", figsize)\n",
    "    latex_params.setdefault(\"usetex\", usetex)\n",
    "\n",
    "    if legend_fontsize is None and legend_match_axes_labelsize:\n",
    "        legend_fontsize = latex_params.get(\"axes_labelsize\", None)\n",
    "\n",
    "    with latex_utils.rc_context_latex(**latex_params):\n",
    "        fig = plt.figure(constrained_layout=True)\n",
    "        gs = fig.add_gridspec(\n",
    "            2, 2,\n",
    "            height_ratios=[1, 1],\n",
    "            width_ratios=[1, 1],\n",
    "            wspace=0.20,\n",
    "            hspace=0.06\n",
    "        )\n",
    "\n",
    "        ax_rank_top = fig.add_subplot(gs[0, 0])\n",
    "        ax_rank_bot = fig.add_subplot(gs[1, 0], sharex=ax_rank_top)\n",
    "\n",
    "        ax_pct_top  = fig.add_subplot(gs[0, 1], sharey=ax_rank_top)\n",
    "        ax_pct_bot  = fig.add_subplot(gs[1, 1], sharey=ax_rank_bot, sharex=ax_pct_top)\n",
    "\n",
    "        def _prettify(ax, show_xticklabels=True):\n",
    "            ax.grid(True, alpha=0.25, linewidth=0.6)\n",
    "            ax.spines[\"top\"].set_visible(False)\n",
    "            ax.spines[\"right\"].set_visible(False)\n",
    "            if not show_xticklabels:\n",
    "                ax.tick_params(axis=\"x\", labelbottom=False)\n",
    "\n",
    "        legend_handles = {}\n",
    "\n",
    "        # left: rank (one line per name)\n",
    "        for name, g in df_rank.groupby(\"name\", sort=False):\n",
    "            g = g.sort_values(\"size_rank\")\n",
    "            color = palette[name]\n",
    "\n",
    "            ln, = ax_rank_top.plot(g[\"size_rank\"], g[\"rmse_test\"], marker=marker, color=color)\n",
    "            ax_rank_bot.plot(g[\"size_rank\"], g[\"rmse_gap\"], marker=marker, color=color)\n",
    "\n",
    "            if name not in legend_handles:\n",
    "                legend_handles[name] = ln\n",
    "\n",
    "        # right: pct (connect per line_id; same color per name)\n",
    "        xmax_pct = int(df_pct[\"dataset_percentage\"].max())\n",
    "\n",
    "        for line_id, g in df_pct.groupby(\"line_id\", sort=False):\n",
    "            g = g.sort_values(\"dataset_percentage\")\n",
    "            name = g[\"name\"].iloc[0]\n",
    "            color = palette[name]\n",
    "\n",
    "            ax_pct_top.plot(g[\"dataset_percentage\"], g[\"rmse_test\"], marker=marker, color=color)\n",
    "            ax_pct_bot.plot(g[\"dataset_percentage\"], g[\"rmse_gap\"],  marker=marker, color=color)\n",
    "\n",
    "            # --- NEW: dashed horizontal extension for MPEM ---\n",
    "            if name == \"MPEM\" and len(g) >= 1:\n",
    "                x_last = int(g[\"dataset_percentage\"].iloc[-1])\n",
    "                if x_last < xmax_pct:\n",
    "                    y_last_top = float(g[\"rmse_test\"].iloc[-1])\n",
    "                    y_last_bot = float(g[\"rmse_gap\"].iloc[-1])\n",
    "\n",
    "                    ax_pct_top.plot([x_last, xmax_pct], [y_last_top, y_last_top],\n",
    "                                    linestyle=\"--\", linewidth=1.2, color=color)\n",
    "                    ax_pct_bot.plot([x_last, xmax_pct], [y_last_bot, y_last_bot],\n",
    "                                    linestyle=\"--\", linewidth=1.2, color=color)\n",
    "\n",
    "        ax_rank_top.set_ylabel(ylabel_test)\n",
    "        ax_rank_bot.set_ylabel(ylabel_gap)\n",
    "        ax_rank_bot.set_xlabel(xlabel_rank)\n",
    "        ax_pct_bot.set_xlabel(xlabel_pct)\n",
    "\n",
    "        ax_pct_top.tick_params(axis=\"y\", labelleft=False)\n",
    "        ax_pct_bot.tick_params(axis=\"y\", labelleft=False)\n",
    "\n",
    "        ax_rank_bot.axhline(0.0, linewidth=1.0, alpha=0.6)\n",
    "        ax_pct_bot.axhline(0.0, linewidth=1.0, alpha=0.6)\n",
    "\n",
    "        roman_upper = {\n",
    "            1:\"I\",2:\"II\",3:\"III\",4:\"IV\",5:\"V\",6:\"VI\",7:\"VII\",8:\"VIII\",9:\"IX\",10:\"X\",\n",
    "            11:\"XI\",12:\"XII\",13:\"XIII\",14:\"XIV\",15:\"XV\",\n",
    "        }\n",
    "        max_rank = int(df_rank[\"size_rank\"].max())\n",
    "        ax_rank_bot.set_xticks(np.arange(1, max_rank + 1))\n",
    "        ax_rank_bot.set_xticklabels([rf\"$\\mathrm{{{roman_upper.get(i, str(i))}}}$\" for i in range(1, max_rank + 1)])\n",
    "\n",
    "        xticks_pct = np.sort(df_pct[\"dataset_percentage\"].unique())\n",
    "        ax_pct_bot.set_xticks(xticks_pct)\n",
    "\n",
    "        if (ymin_test is not None) or (ymax_test is not None):\n",
    "            ax_rank_top.set_ylim(bottom=ymin_test, top=ymax_test)\n",
    "        if (ymin_gap is not None) or (ymax_gap is not None):\n",
    "            ax_rank_bot.set_ylim(bottom=ymin_gap, top=ymax_gap)\n",
    "\n",
    "        _prettify(ax_rank_top, show_xticklabels=False)\n",
    "        _prettify(ax_rank_bot, show_xticklabels=True)\n",
    "        _prettify(ax_pct_top,  show_xticklabels=False)\n",
    "        _prettify(ax_pct_bot,  show_xticklabels=True)\n",
    "\n",
    "        if show_column_titles and column_titles is not None:\n",
    "            ax_rank_top.set_title(column_titles[0])\n",
    "            ax_pct_top.set_title(column_titles[1])\n",
    "\n",
    "        fig.suptitle(title if title is not None else r\"Test RMSE and (Test $-$ Train) gap\")\n",
    "        fig.align_ylabels([ax_rank_top, ax_rank_bot])\n",
    "\n",
    "        a, b, c, d = panel_labels\n",
    "        ax_rank_top.text(0.5, panel_label_y_top, f\"({a})\", transform=ax_rank_top.transAxes,\n",
    "                         ha=\"center\", va=\"top\", clip_on=False)\n",
    "        ax_rank_bot.text(0.5, panel_label_y_bot, f\"({b})\", transform=ax_rank_bot.transAxes,\n",
    "                         ha=\"center\", va=\"top\", clip_on=False)\n",
    "        ax_pct_top.text(0.5, panel_label_y_top, f\"({c})\", transform=ax_pct_top.transAxes,\n",
    "                        ha=\"center\", va=\"top\", clip_on=False)\n",
    "        ax_pct_bot.text(0.5, panel_label_y_bot, f\"({d})\", transform=ax_pct_bot.transAxes,\n",
    "                        ha=\"center\", va=\"top\", clip_on=False)\n",
    "\n",
    "        leg_labels = [nm for nm in legend_order if nm in legend_handles]\n",
    "        leg_handles = [legend_handles[nm] for nm in leg_labels]\n",
    "\n",
    "        if legend_out:\n",
    "            fig.legend(\n",
    "                leg_handles, leg_labels,\n",
    "                frameon=False,\n",
    "                loc=\"center left\",\n",
    "                bbox_to_anchor=(1.01, 0.5),\n",
    "                ncol=legend_ncol,\n",
    "                fontsize=legend_fontsize,\n",
    "            )\n",
    "        else:\n",
    "            ax_pct_top.legend(\n",
    "                leg_handles, leg_labels,\n",
    "                frameon=False,\n",
    "                loc=\"best\",\n",
    "                fontsize=legend_fontsize,\n",
    "            )\n",
    "\n",
    "        # Create directory if it doesn't exist\n",
    "        os.makedirs(os.path.dirname(store_to), exist_ok=True)\n",
    "        fig.savefig(store_to, bbox_inches=\"tight\")\n",
    "        plt.show()\n",
    "\n",
    "    return fig, (ax_rank_top, ax_rank_bot, ax_pct_top, ax_pct_bot), {\"rank\": df_rank, \"pct\": df_pct}, palette"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e5df350",
   "metadata": {},
   "outputs": [],
   "source": [
    "palette_order = [\"ActuationNet\", \"PotentialNet\", \"DirectNet\", \"DirectGBT\", \"MPEM\"]\n",
    "sns_pal = sns.color_palette(\"deep\", n_colors=len(palette_order))\n",
    "name_to_color = {nm: sns_pal[i] for i, nm in enumerate(palette_order)}\n",
    "\n",
    "\n",
    "fig_rmse, axes_rmse, dfs_rmse, rmse_palette = plot_train_test_rmse_rank_and_pct_suite_latex(\n",
    "    training_package,\n",
    "    test_package,\n",
    "    models_rank=full_models,\n",
    "    models_pct=large_models,\n",
    "    latex_params=latex_utils.latex_prms_2img,\n",
    "    usetex=True,\n",
    "    title=\"\",\n",
    "    name_to_color=name_to_color,\n",
    "    ylabel_gap=r\"Gap RMSE (mT)\",\n",
    "    ymin_test=0.0,\n",
    "    store_to=plot_dir + \"rmse_suite.pdf\",\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "calibration",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
