{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2653ef0b",
   "metadata": {},
   "source": [
    "# Train neural net for actuation matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9d2ca7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import os\n",
    "import sys\n",
    "import gc\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import wandb\n",
    "import copy\n",
    "\n",
    "\n",
    "cwd = os.getcwd() \n",
    "parent_dir = os.path.dirname(cwd)\n",
    "base_dir = os.path.dirname(parent_dir)\n",
    "\n",
    "src_dir = base_dir + \"/src\"\n",
    "\n",
    "sys.path.insert(0, src_dir)\n",
    "from calibration import ActuationNet, DirectNet, PotentialNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f62859c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#####################################\n",
    "########### Which eMNS? #############\n",
    "#####################################\n",
    "emns = \"octomag\" # \"octomag\" or \"navion\"\n",
    "\n",
    "if emns.lower() == \"octomag\":\n",
    "    data_dir = base_dir + \"/data/octomag_data/split_dataset\"\n",
    "    params_dir = cwd + \"/params\"\n",
    "elif emns.lower() == \"navion\":\n",
    "    data_dir = base_dir + \"/data/navion_data/split_dataset\"\n",
    "    params_dir = cwd + \"/params_navion\"\n",
    "else:\n",
    "    raise ValueError(\"eMNS type not recognized. Please use 'octomag' or 'navion'.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "457defa7",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e43f2708",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the data percentage\n",
    "data_percentage = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6484d201",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data_path = data_dir + \"/training_data\" + (\".pkl\" if data_percentage == 100 else \"_\"+str(data_percentage)+\".pkl\")\n",
    "validation_data_path = data_dir + \"/validation_data\" + (\".pkl\" if data_percentage == 100 else \"_\"+str(data_percentage)+\".pkl\")\n",
    "test_data_path = data_dir + \"/test_data.pkl\"\n",
    "\n",
    "training_data = pd.read_pickle(training_data_path)\n",
    "validation_data = pd.read_pickle(validation_data_path)\n",
    "test_data = pd.read_pickle(test_data_path)\n",
    "\n",
    "# Select only relevant columns\n",
    "pos_cols = [\"x\", \"y\", \"z\"]\n",
    "field_cols = [\"Bx\", \"By\", \"Bz\"]\n",
    "em_cols = [col for col in training_data.columns if col.startswith(\"em_\")]\n",
    "\n",
    "training_data = training_data[pos_cols + field_cols + em_cols]\n",
    "validation_data = validation_data[pos_cols + field_cols + em_cols]\n",
    "test_data = test_data[pos_cols + field_cols + em_cols]\n",
    "\n",
    "print(\"Data info:\")\n",
    "print(f\"Training data points: {len(training_data)}\")\n",
    "print(f\"Validation data points: {len(validation_data)}\")\n",
    "print(f\"Test data points: {len(test_data)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8c30fe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Position distributions: x, y, z\n",
    "bins = 50  # adjust as needed\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 4), sharey=False)\n",
    "pos_names = [\"x\", \"y\", \"z\"]\n",
    "datasets = [training_data, validation_data, test_data]\n",
    "labels = [\"train\", \"val\", \"test\"]\n",
    "colors = [\"C0\", \"C1\", \"C2\"]\n",
    "\n",
    "for i, col in enumerate(pos_names):\n",
    "    ax = axes[i]\n",
    "    for ds, lab, colr in zip(datasets, labels, colors):\n",
    "        ax.hist(ds[col], bins=bins, alpha=0.4, label=lab, density=True, color=colr)\n",
    "    ax.set_title(f\"{col} distribution\")\n",
    "    ax.set_xlabel(col)\n",
    "    ax.set_ylabel(\"Density\")\n",
    "    if i == 0:\n",
    "        ax.legend()\n",
    "\n",
    "fig.suptitle(\"Position distributions (x, y, z) for train/val/test\", y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Field component distributions: Bx, By, Bz\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 4), sharey=False)\n",
    "field_names = [\"Bx\", \"By\", \"Bz\"]\n",
    "\n",
    "for i, col in enumerate(field_names):\n",
    "    ax = axes[i]\n",
    "    for ds, lab, colr in zip(datasets, labels, colors):\n",
    "        ax.hist(ds[col], bins=bins, alpha=0.4, label=lab, density=True, color=colr)\n",
    "    ax.set_title(f\"{col} distribution\")\n",
    "    ax.set_xlabel(col)\n",
    "    ax.set_ylabel(\"Density\")\n",
    "    if i == 0:\n",
    "        ax.legend()\n",
    "\n",
    "fig.suptitle(\"Field component distributions (Bx, By, Bz) for train/val/test\", y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Field norm distribution: |B|\n",
    "# compute norms\n",
    "train_B_norm = np.linalg.norm(training_data[field_cols].to_numpy(), axis=1)\n",
    "val_B_norm   = np.linalg.norm(validation_data[field_cols].to_numpy(), axis=1)\n",
    "test_B_norm  = np.linalg.norm(test_data[field_cols].to_numpy(), axis=1)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(6, 4))\n",
    "\n",
    "ax.hist(train_B_norm, bins=bins, alpha=0.4, density=True, label=\"train\", color=\"C0\")\n",
    "ax.hist(val_B_norm,   bins=bins, alpha=0.4, density=True, label=\"val\",   color=\"C1\")\n",
    "ax.hist(test_B_norm,  bins=bins, alpha=0.4, density=True, label=\"test\",  color=\"C2\")\n",
    "\n",
    "ax.set_title(\"|B| distribution for train/val/test\")\n",
    "ax.set_xlabel(\"|B|\")\n",
    "ax.set_ylabel(\"Density\")\n",
    "ax.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "565f7266",
   "metadata": {},
   "source": [
    "## Setup normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78704616",
   "metadata": {},
   "outputs": [],
   "source": [
    "normalize_pos = False # We did not normalize position\n",
    "normalize_field = True\n",
    "\n",
    "normalization_params = {\"input\": None, \"output\": None}\n",
    "\n",
    "if normalize_pos:\n",
    "    pos_mean = training_data[pos_cols].mean().to_numpy()  # shape (3,)\n",
    "    pos_std  = training_data[pos_cols].std().to_numpy()\n",
    "    normalization_params[\"input\"] = {\n",
    "        \"mean\": pos_mean,\n",
    "        \"std\": pos_std,\n",
    "    }\n",
    "\n",
    "if normalize_field:\n",
    "    field_mean = training_data[field_cols].mean().to_numpy()  # shape (3,)\n",
    "    field_std  = training_data[field_cols].std().to_numpy()\n",
    "    normalization_params[\"output\"] = {\n",
    "        \"mean\": field_mean,\n",
    "        \"std\": field_std,\n",
    "    }\n",
    "\n",
    "if normalize_pos:\n",
    "    print(\"Position normalization parameters:\")\n",
    "    print(f\"Mean: {normalization_params['input']['mean']}\")\n",
    "    print(f\"Std: {normalization_params['input']['std']}\")\n",
    "else:\n",
    "    print(\"Position normalization not applied.\")\n",
    "\n",
    "if normalize_field:\n",
    "    print(\"Field normalization parameters:\")\n",
    "    print(f\"Mean: {normalization_params['output']['mean']}\")\n",
    "    print(f\"Std: {normalization_params['output']['std']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0b5f5e9",
   "metadata": {},
   "source": [
    "## Build Torch Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31f13064",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a torch Dataset class\n",
    "\n",
    "class MagneticDataset(Dataset):\n",
    "    def __init__(self, pos, currents, field):\n",
    "        \"\"\"\n",
    "        pos:      np.ndarray (N, 3)\n",
    "        currents: np.ndarray (N, 8)\n",
    "        field:    np.ndarray (N, 3)\n",
    "        \"\"\"\n",
    "\n",
    "        self.pos = torch.from_numpy(pos.astype(np.float32))\n",
    "        self.curr = torch.from_numpy(currents.astype(np.float32))\n",
    "        self.field = torch.from_numpy(field.astype(np.float32))\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.pos.shape[0]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.pos[idx], self.curr[idx], self.field[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1aed1f37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract data to numpy arrays\n",
    "pos_train   = training_data[pos_cols].to_numpy()      # (N_train, 3)\n",
    "field_train = training_data[field_cols].to_numpy()    # (N_train, 3)\n",
    "curr_train  = training_data[em_cols].to_numpy()       # (N_train, 8)\n",
    "\n",
    "pos_val   = validation_data[pos_cols].to_numpy()\n",
    "field_val = validation_data[field_cols].to_numpy()\n",
    "curr_val  = validation_data[em_cols].to_numpy()\n",
    "\n",
    "pos_test   = test_data[pos_cols].to_numpy()\n",
    "field_test = test_data[field_cols].to_numpy()\n",
    "curr_test  = test_data[em_cols].to_numpy()\n",
    "\n",
    "\n",
    "# Build datasets\n",
    "train_dataset = MagneticDataset(\n",
    "    pos_train,\n",
    "    curr_train,\n",
    "    field_train\n",
    ")\n",
    "\n",
    "val_dataset = MagneticDataset(\n",
    "    pos_val,\n",
    "    curr_val,\n",
    "    field_val\n",
    ")\n",
    "\n",
    "test_dataset = MagneticDataset(\n",
    "    pos_test,\n",
    "    curr_test,\n",
    "    field_test\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e349f583",
   "metadata": {},
   "source": [
    "## Training ðŸŒŠðŸ„â€â™‚ï¸ðŸ¤™"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d478e274",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the model\n",
    "model_class = PotentialNet\n",
    "hidden_dims = (512, 512, 512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27f44486",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_percentage_name = str(data_percentage)\n",
    "structure_name = \"x\".join([str(dim) for dim in hidden_dims])\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model = model_class(\n",
    "    name=model_class.__name__ + \"_\" + data_percentage_name + \"_\" + structure_name,\n",
    "    hidden_dims=hidden_dims,\n",
    "    num_coils=8 if emns.lower() == \"octomag\" else 3,\n",
    "    position_normalization_dict=normalization_params[\"input\"],\n",
    "    field_normalization_dict=normalization_params[\"output\"],\n",
    ").to(device)\n",
    "\n",
    "print(\"Model name is: \" + model.name)\n",
    "\n",
    "\n",
    "batch_size = 512\n",
    "num_epochs = 10000\n",
    "adam_lr = 1e-3\n",
    "\n",
    "early_stopping_patience = 10\n",
    "early_stopping_threshold = 0.001\n",
    "\n",
    "# Adapt patience based on reduced dataset\n",
    "# Load full dataset\n",
    "adjusted_patience = max(1, int(early_stopping_patience / data_percentage * 100))\n",
    "early_stopping_patience = adjusted_patience\n",
    "print(f\"Adjusted early stopping patience: {early_stopping_patience}\")\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=adam_lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05c7e314",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset to DataLoader\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader   = DataLoader(val_dataset,   batch_size=batch_size, shuffle=False)\n",
    "test_loader  = DataLoader(test_dataset,  batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Init wandb\n",
    "wandb.init(\n",
    "    project=\"ReproducibleMagneticNetworks\",\n",
    "    name=model.name,\n",
    "    config={\n",
    "        \"batch_size\": batch_size,\n",
    "        \"num_epochs\": num_epochs,\n",
    "        \"lr\": adam_lr,\n",
    "        \"model\": model.name,\n",
    "        \"normalize_pos\": normalize_pos,\n",
    "        \"normalize_field\": normalize_field,\n",
    "    },\n",
    ")\n",
    "\n",
    "wandb.watch(model, criterion, log=\"all\", log_freq=100)\n",
    "\n",
    "# --- early stopping state ---\n",
    "baseline_rmse = None          # first RMSE of current run\n",
    "plateau_counter = 0\n",
    "\n",
    "# --- best model tracking (for saving) ---\n",
    "best_val_rmse = float('inf')\n",
    "best_state_dict = copy.deepcopy(model.state_dict())\n",
    "best_epoch = 0\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "\n",
    "    # accumulators for norms (train)\n",
    "    J_frob_train_sum = 0.0\n",
    "    b_norm_train_sum = 0.0\n",
    "    train_batches = 0\n",
    "    has_b_train = False\n",
    "\n",
    "    # accumulator for train SE\n",
    "    se_sum_train = 0.0\n",
    "\n",
    "    for pos_batch, curr_batch, field_batch in train_loader:\n",
    "        pos_batch   = pos_batch.to(device)\n",
    "        curr_batch  = curr_batch.to(device)\n",
    "        field_batch = field_batch.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        out = model(pos_batch, curr_batch)\n",
    "\n",
    "        # Handle (field, J) or (field, J, b)\n",
    "        J_batch = None\n",
    "        b_batch = None\n",
    "\n",
    "        if isinstance(out, (list, tuple)):\n",
    "            if len(out) == 3:\n",
    "                field_pred, J_batch, b_batch = out\n",
    "            elif len(out) == 2:\n",
    "                field_pred, J_batch = out\n",
    "            else:\n",
    "                field_pred = out[0]\n",
    "        else:\n",
    "            field_pred = out\n",
    "\n",
    "        loss = criterion(field_pred, field_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.item() * pos_batch.size(0)\n",
    "        train_batches += 1\n",
    "\n",
    "        # ---- Frobenius norm of J and L2 norm of b over batch ----\n",
    "        if J_batch is not None:\n",
    "            # J_batch: (B, 3, 8) -> per-sample Frobenius, then mean over B\n",
    "            J_frob_batch = J_batch.view(J_batch.size(0), -1).norm(dim=1).mean().item()\n",
    "            J_frob_train_sum += J_frob_batch\n",
    "\n",
    "        if b_batch is not None:\n",
    "            # b_batch: (B, 3) -> per-sample L2 norm, then mean over B\n",
    "            b_norm_batch = b_batch.norm(dim=1).mean().item()\n",
    "            b_norm_train_sum += b_norm_batch\n",
    "            has_b_train = True\n",
    "\n",
    "        pred_np_train = field_pred.detach().cpu().numpy()  # (B, 3)\n",
    "        true_np_train = field_batch.detach().cpu().numpy() # (B, 3)\n",
    "\n",
    "        se_sum_train += np.sum((pred_np_train - true_np_train) ** 2)\n",
    "\n",
    "    train_loss /= len(train_dataset)\n",
    "    J_frob_train = J_frob_train_sum / train_batches if train_batches > 0 else 0.0\n",
    "    b_norm_train = b_norm_train_sum / train_batches if (train_batches > 0 and has_b_train) else 0.0\n",
    "\n",
    "    # train RMSE\n",
    "    n_vals_train = len(train_dataset)\n",
    "    mse_train  = se_sum_train / n_vals_train\n",
    "    rmse_train = np.sqrt(mse_train)\n",
    "\n",
    "    # ----- validation -----\n",
    "    model.eval()\n",
    "    val_loss = 0.0 \n",
    "    se_sum = 0.0\n",
    "\n",
    "    # accumulators for norms (val)\n",
    "    J_frob_val_sum = 0.0\n",
    "    b_norm_val_sum = 0.0\n",
    "    val_batches = 0\n",
    "    has_b_val = False\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for pos_batch, curr_batch, field_batch in val_loader:\n",
    "            pos_batch   = pos_batch.to(device)\n",
    "            curr_batch  = curr_batch.to(device)\n",
    "            field_batch = field_batch.to(device)\n",
    "\n",
    "            out = model(pos_batch, curr_batch)\n",
    "            J_batch = None\n",
    "            b_batch = None\n",
    "\n",
    "            if isinstance(out, (list, tuple)):\n",
    "                if len(out) == 3:\n",
    "                    field_pred, J_batch, b_batch = out\n",
    "                elif len(out) == 2:\n",
    "                    field_pred, J_batch = out\n",
    "                else:\n",
    "                    field_pred = out[0]\n",
    "            else:\n",
    "                field_pred = out\n",
    "\n",
    "            loss = criterion(field_pred, field_batch)\n",
    "            val_loss += loss.item() * pos_batch.size(0)\n",
    "            val_batches += 1\n",
    "\n",
    "            # norms\n",
    "            if J_batch is not None:\n",
    "                J_frob_batch = J_batch.view(J_batch.size(0), -1).norm(dim=1).mean().item()\n",
    "                J_frob_val_sum += J_frob_batch\n",
    "            if b_batch is not None:\n",
    "                b_norm_batch = b_batch.norm(dim=1).mean().item()\n",
    "                b_norm_val_sum += b_norm_batch\n",
    "                has_b_val = True\n",
    "\n",
    "            pred_np  = field_pred.cpu().numpy()    # (B, 3)\n",
    "            true_np  = field_batch.cpu().numpy()   # (B, 3)\n",
    "\n",
    "            se_sum += np.sum((pred_np - true_np) ** 2)\n",
    "\n",
    "    val_loss /= len(val_dataset)\n",
    "    J_frob_val = J_frob_val_sum / val_batches if val_batches > 0 else 0.0\n",
    "    b_norm_val = b_norm_val_sum / val_batches if (val_batches > 0 and has_b_val) else 0.0\n",
    "\n",
    "    # MSE / RMSE\n",
    "    n_vals = len(val_dataset)  # N samples\n",
    "    mse  = se_sum / n_vals\n",
    "    rmse = np.sqrt(mse)\n",
    "\n",
    "    # -------- best model tracking --------\n",
    "    if rmse < best_val_rmse:\n",
    "        best_val_rmse = rmse\n",
    "        best_state_dict = copy.deepcopy(model.state_dict())\n",
    "        best_epoch = epoch + 1\n",
    "\n",
    "    # -------- early stopping vs first --------\n",
    "    if baseline_rmse is None:\n",
    "        # first epoch initializes the baseline\n",
    "        baseline_rmse = rmse\n",
    "        plateau_counter = 0\n",
    "    else:\n",
    "        improvement = baseline_rmse - rmse\n",
    "        if improvement > early_stopping_threshold:\n",
    "            # improved by more than threshold vs baseline -> reset run\n",
    "            baseline_rmse = rmse\n",
    "            plateau_counter = 0\n",
    "        else:\n",
    "            # no significant improvement vs baseline\n",
    "            plateau_counter += 1\n",
    "\n",
    "    if plateau_counter >= early_stopping_patience:\n",
    "        print(\n",
    "            f\"Early stopping at epoch {epoch+1} \"\n",
    "            f\"due to no > {early_stopping_threshold:.3g} improvement in val RMSE \"\n",
    "            f\"for {early_stopping_patience} epochs.\"\n",
    "        )\n",
    "        break\n",
    "\n",
    "    # log to wandb\n",
    "    log_dict = {\n",
    "        \"epoch\": epoch + 1,\n",
    "        \"train_loss_norm\": train_loss,\n",
    "        \"val_loss_norm\": val_loss,\n",
    "        \"train_RMSE\": rmse_train,\n",
    "        \"val_RMSE\": rmse,\n",
    "        \"J_frob_train\": J_frob_train,\n",
    "        \"J_frob_val\": J_frob_val,\n",
    "    }\n",
    "    if has_b_train or has_b_val:\n",
    "        log_dict[\"b_norm_train\"] = b_norm_train\n",
    "        log_dict[\"b_norm_val\"] = b_norm_val\n",
    "\n",
    "    wandb.log(log_dict)\n",
    "\n",
    "    print(\n",
    "        f\"Epoch {epoch+1:03d} | \"\n",
    "        f\"train_loss (norm) = {train_loss:.6f} | \"\n",
    "        f\"val_loss (norm) = {val_loss:.6f} | \"\n",
    "        f\"train_RMSE = {rmse_train:.3e} | \"\n",
    "        f\"val_RMSE = {rmse:.3e}\"\n",
    "    )\n",
    "\n",
    "# ---- Restore best weights and save best model ----\n",
    "model.load_state_dict(best_state_dict)\n",
    "\n",
    "model_save_path = model.save(folder=params_dir)\n",
    "print(f\"Best model (epoch {best_epoch}, val_RMSE={best_val_rmse:.3e}) saved to: {model_save_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "101c599d",
   "metadata": {},
   "source": [
    "## Check how it performs on validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8253b32",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "\n",
    "validation_data = test_data.copy()\n",
    "pos_val  = validation_data[[\"x\", \"y\", \"z\"]].to_numpy().astype(np.float32)\n",
    "curr_val = validation_data[em_cols].to_numpy().astype(np.float32)\n",
    "\n",
    "pos_tensor  = torch.from_numpy(pos_val).to(device)   # (N, 3)\n",
    "curr_tensor = torch.from_numpy(curr_val).to(device)  # (N, 8)\n",
    "\n",
    "with torch.no_grad():\n",
    "    out = model(pos_tensor, curr_tensor)\n",
    "\n",
    "    if isinstance(out, (list, tuple)):\n",
    "        field_pred = out[0]\n",
    "    else:\n",
    "        field_pred = out\n",
    "\n",
    "    pred_np = field_pred.cpu().numpy()\n",
    "\n",
    "\n",
    "validation_data[\"Bx_pred\"] = pred_np[:, 0]\n",
    "validation_data[\"By_pred\"] = pred_np[:, 1]\n",
    "validation_data[\"Bz_pred\"] = pred_np[:, 2]\n",
    "\n",
    "validation_data[\"B_mag\"] = np.sqrt(\n",
    "    validation_data[\"Bx\"]**2 +\n",
    "    validation_data[\"By\"]**2 +\n",
    "    validation_data[\"Bz\"]**2\n",
    ")\n",
    "validation_data[\"B_mag_pred\"] = np.sqrt(\n",
    "    validation_data[\"Bx_pred\"]**2 +\n",
    "    validation_data[\"By_pred\"]**2 +\n",
    "    validation_data[\"Bz_pred\"]**2\n",
    ")\n",
    "\n",
    "# Compute errors\n",
    "validation_data[\"Bx_error\"] = validation_data[\"Bx_pred\"] - validation_data[\"Bx\"]\n",
    "validation_data[\"By_error\"] = validation_data[\"By_pred\"] - validation_data[\"By\"]\n",
    "validation_data[\"Bz_error\"] = validation_data[\"Bz_pred\"] - validation_data[\"Bz\"]\n",
    "\n",
    "validation_data[\"B_mag_error\"] = validation_data[\"B_mag_pred\"] - validation_data[\"B_mag\"]\n",
    "validation_data[\"B_mag_error_rel\"] = validation_data[\"B_mag_error\"] / validation_data[\"B_mag\"].replace(0, np.nan)\n",
    "\n",
    "validation_data[\"B_angle_error\"] = np.arccos(\n",
    "    (validation_data[\"Bx\"] * validation_data[\"Bx_pred\"] +\n",
    "     validation_data[\"By\"] * validation_data[\"By_pred\"] +\n",
    "     validation_data[\"Bz\"] * validation_data[\"Bz_pred\"]) /\n",
    "    (validation_data[\"B_mag\"] * validation_data[\"B_mag_pred\"]).replace(0, np.nan)\n",
    ") * (180.0 / np.pi)  # in degrees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33f4d17d",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "\n",
    "rel_err = validation_data[\"B_mag_error_rel\"].to_numpy()\n",
    "abs_rel_err = np.abs(rel_err)\n",
    "\n",
    "# avoid log(0): clip zeros to a tiny positive value\n",
    "eps = 1e-8\n",
    "abs_rel_err = np.clip(abs_rel_err, eps, None)\n",
    "\n",
    "# log-spaced bin edges, e.g. from ~min to ~max\n",
    "bin_min = max(abs_rel_err.min(), 1e-6)\n",
    "bin_max = abs_rel_err.max()\n",
    "bins = np.logspace(np.log10(bin_min), np.log10(bin_max), 40)  # 40 log bins\n",
    "\n",
    "axes[0].hist(\n",
    "    abs_rel_err,\n",
    "    bins=bins,\n",
    "    alpha=0.7,\n",
    "    color=\"blue\",\n",
    "    density=False,\n",
    ")\n",
    "axes[0].set_xscale(\"log\")  # so bins like [1e-3, 1e-2], [1e-2, 1e-1], ...\n",
    "axes[0].set_title(\"Histogram of |B| Relative Error\")\n",
    "axes[0].set_xlabel(\"|B_mag_relative_error|\")\n",
    "axes[0].set_ylabel(\"Count\")\n",
    "axes[0].grid(False)\n",
    "\n",
    "\n",
    "axes[1].hist(\n",
    "    validation_data[\"B_angle_error\"],\n",
    "    bins=100,\n",
    "    alpha=0.7,\n",
    "    color=\"blue\",\n",
    "    density=False,\n",
    ")\n",
    "axes[1].set_title(\"Histogram of B Angle Error\")\n",
    "axes[1].set_xlabel(\"B_angle Error (degrees)\")\n",
    "axes[1].set_ylabel(\"Count\")\n",
    "axes[1].grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a688360",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute |B_true| and distance from center r\n",
    "B_true = validation_data[[\"Bx\", \"By\", \"Bz\"]].to_numpy()\n",
    "pos    = validation_data[[\"x\", \"y\", \"z\"]].to_numpy()\n",
    "\n",
    "B_mag  = np.linalg.norm(B_true, axis=1)   # (N,)\n",
    "r      = np.linalg.norm(pos, axis=1)      # (N,)\n",
    "\n",
    "# errors\n",
    "err_mag_rel = np.abs(validation_data[\"B_mag_error_rel\"].to_numpy())\n",
    "err_angle   = np.abs(validation_data[\"B_angle_error\"].to_numpy())  # or keep sign if you want\n",
    "\n",
    "# Filter out too small |B|\n",
    "B_threshold = 0.0  # mT\n",
    "mask = B_mag >= B_threshold\n",
    "\n",
    "B_mag      = B_mag[mask]\n",
    "r          = r[mask]\n",
    "err_mag_rel = err_mag_rel[mask]\n",
    "err_angle   = err_angle[mask]\n",
    "\n",
    "# Binning\n",
    "n_bins = 100\n",
    "n_bins_B = n_bins\n",
    "n_bins_r = n_bins\n",
    "\n",
    "\n",
    "B_min, B_max = B_mag.min(), B_mag.max()\n",
    "r_min, r_max = r.min(), r.max()\n",
    "\n",
    "B_edges = np.linspace(B_min, B_max, n_bins_B + 1)\n",
    "r_edges = np.linspace(r_min, r_max, n_bins_r + 1)\n",
    "\n",
    "# --------- 2D binned mean relative magnitude error ---------\n",
    "err_sum_mag, _, _ = np.histogram2d(B_mag, r, bins=[B_edges, r_edges], weights=err_mag_rel)\n",
    "count_mag,    _, _ = np.histogram2d(B_mag, r, bins=[B_edges, r_edges])\n",
    "\n",
    "mean_err_mag = np.zeros_like(err_sum_mag)\n",
    "mask_mag = count_mag > 0\n",
    "mean_err_mag[mask_mag] = err_sum_mag[mask_mag] / count_mag[mask_mag]\n",
    "mean_err_mag[~mask_mag] = np.nan\n",
    "\n",
    "# --------- 2D binned mean angle error ---------\n",
    "err_sum_ang, _, _ = np.histogram2d(B_mag, r, bins=[B_edges, r_edges], weights=err_angle)\n",
    "count_ang,   _, _ = np.histogram2d(B_mag, r, bins=[B_edges, r_edges])\n",
    "\n",
    "mean_err_ang = np.zeros_like(err_sum_ang)\n",
    "mask_ang = count_ang > 0\n",
    "mean_err_ang[mask_ang] = err_sum_ang[mask_ang] / count_ang[mask_ang]\n",
    "mean_err_ang[~mask_ang] = np.nan\n",
    "\n",
    "# --------- Plot side by side ---------\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 6), sharex=True, sharey=True)\n",
    "\n",
    "# Left: relative |B| error\n",
    "pcm1 = axes[0].pcolormesh(B_edges, r_edges, mean_err_mag.T, shading=\"auto\")\n",
    "cb1 = fig.colorbar(pcm1, ax=axes[0])\n",
    "cb1.set_label(\"Mean |B| relative error\")\n",
    "axes[0].set_xlabel(\"|B_true|\")\n",
    "axes[0].set_ylabel(\"â€–positionâ€– (m)\")\n",
    "axes[0].set_title(\"Relative |B| error vs |B| and radius\")\n",
    "\n",
    "# Right: angle error\n",
    "pcm2 = axes[1].pcolormesh(B_edges, r_edges, mean_err_ang.T, shading=\"auto\")\n",
    "cb2 = fig.colorbar(pcm2, ax=axes[1])\n",
    "cb2.set_label(\"Mean angle error (deg)\")\n",
    "axes[1].set_xlabel(\"|B_true|\")\n",
    "axes[1].set_title(\"Angle error vs |B| and radius\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "calibration",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
